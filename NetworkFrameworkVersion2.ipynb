{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Add support for batch sizes greater than 1 (store and wait to update) DONE\n",
    "- Add softmax activation function DONE\n",
    "- Fix initialization (it sucks right now, but negatives are confusing)\n",
    "- Clean up the code\n",
    "- Add GPU support (CUDA, AMD?)\n",
    "- Add support for tensor in, tensor out? (rework einsums to be more general)\n",
    "- Add progress bar (I already made this!!!)\n",
    "- Add support for multiple different cost functions\n",
    "- Add cross-entropy cost function (log-loss?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork():\n",
    "    last_layer = None\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    def __init__(self, layer_tree=None):\n",
    "        self.last_layer = layer_tree\n",
    "    \n",
    "    #Call forward method on last layer\n",
    "    def forward(self, X):\n",
    "        return self.last_layer.forward(X)\n",
    "    \n",
    "    #Call back method on last layer\n",
    "    def back(self, Y):\n",
    "        assert type(self.last_layer).__name__ != \"nnLayer\"\n",
    "        if type(self.last_layer).__name__ == \"nnOutputLayer\": #Multilayer network\n",
    "            return self.last_layer.back(Y)\n",
    "        else: #The rare case of a single layer network\n",
    "            #Calculate error vector\n",
    "            error_vector = self.last_layer.activations(self.last_layer.weighted_sum_store) - Y\n",
    "            #Calculate cost gradient\n",
    "            cost_gradient = error_vector\n",
    "            return self.last_layer.back(cost_gradient)\n",
    "    \n",
    "    #Train using backpropogation reinforcement learning\n",
    "    def train(self, X_Array, Y_Array, epochs=1, batch_size=100):\n",
    "        assert len(X_Array) == len(Y_Array)\n",
    "        assert epochs >= 1\n",
    "        for j in range(0, epochs):\n",
    "            print(\"Epoch: \" + str(j))\n",
    "            for i in range(0, len(X_Array)):\n",
    "                self.forward(X_Array[i])\n",
    "                self.back(Y_Array[i])\n",
    "                if i % batch_size == 0:\n",
    "                    #Fix batch size division for batches underfull\n",
    "                    self.last_layer.flush_updates(multiplier=(self.learning_rate / batch_size))\n",
    "            self.last_layer.flush_updates(multiplier=(self.learning_rate / batch_size))\n",
    "        return True\n",
    "    \n",
    "    #What the model considers to be layers are what the user considers to be the transformations between them\n",
    "    #For n user-defined layers, the model creates n-1 layer objects including one input and one output\n",
    "    \n",
    "    #Create a model whose layers all use the same activation function\n",
    "    def create_homogenous_model(self, size_array, activation_function, learning_rate=0.01):\n",
    "        layer_storage = None\n",
    "        for i in range(0, len(size_array) - 1):\n",
    "            if i == 0: #Input layer\n",
    "                #input_dim, output_dim, activation_function\n",
    "                layer_store = nnInputLayer(size_array[i], size_array[i + 1], activation_function)\n",
    "            elif i == len(size_array) - 2: #Output layer\n",
    "                #input_dim, output_dim, activation_function, previous_layer\n",
    "                layer_store = nnOutputLayer(size_array[i], size_array[i + 1], activation_function, layer_store)\n",
    "            else: #Middle layer\n",
    "                #input_dim, output_dim, activation_function, previous_layer\n",
    "                layer_store = nnLayer(size_array[i], size_array[i + 1], activation_function, layer_store)\n",
    "        self.last_layer = layer_store\n",
    "        return self\n",
    "    \n",
    "    def total_cost(self, X_Array, Y_Array):\n",
    "        total_cost = 0\n",
    "        for i in range(0, len(X_Array)):\n",
    "            error = self.forward(X_Array[i]) - Y_Array[i]\n",
    "            cost = np.dot(error, error)\n",
    "            total_cost += cost\n",
    "        return total_cost\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnLayer(object):\n",
    "    weights = None\n",
    "    biases = None\n",
    "    activation_function = None\n",
    "    previous_activation_store = None\n",
    "    weighted_sum_store = None\n",
    "    previous_layer = None\n",
    "    weight_jacobian_store = None\n",
    "    bias_jacobian_store = None\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation_function, previous_layer):\n",
    "        self.activation_function = activation_function\n",
    "        #self.weights = np.random.normal(0, 1, (output_dim, input_dim))\n",
    "        #self.biases = np.random.normal(0, 1, (output_dim))\n",
    "        self.weights = np.random.rand(output_dim, input_dim) / output_dim\n",
    "        self.biases = np.random.rand(output_dim) / output_dim\n",
    "        #self.weights = np.random.normal(0.5, 0.5, (output_dim, input_dim))\n",
    "        #self.biases = np.random.normal(0.5, 0.5, (output_dim))\n",
    "        self.previous_activation_store = np.zeros(input_dim)\n",
    "        self.weighted_sum_store = np.zeros(output_dim)\n",
    "        self.previous_layer = previous_layer\n",
    "        self.weight_jacobian_store = np.zeros((output_dim, input_dim))\n",
    "        self.bias_jacobian_store = np.zeros((output_dim))\n",
    "    \n",
    "    #Protocol for forward propagation\n",
    "    def forward(self, X):\n",
    "        self.previous_activation_store = self.previous_layer.forward(X)\n",
    "        self.weighted_sum_store = self.weighted_sum(self.previous_activation_store)\n",
    "        return self.activations(self.weighted_sum_store)\n",
    "    \n",
    "    #Protocol for back propagation\n",
    "    def back(self, forward_jacobian):\n",
    "        #Calculate reusable jacobian\n",
    "        reusable = self.calculate_reusable_jacobian(forward_jacobian)\n",
    "        #Calculate weight jacobian and update\n",
    "        weight_jacobian = self.calculate_weight_jacobian(reusable)\n",
    "        self.weight_jacobian_store += weight_jacobian\n",
    "        #Calculate bias jacobian and update\n",
    "        bias_jacobian = self.calculate_bias_jacobian(reusable)\n",
    "        self.bias_jacobian_store += bias_jacobian\n",
    "        #Calculate back/forward jacobian\n",
    "        back_jacobian = self.calculate_back_jacobian(reusable)\n",
    "        #Call back() on previous layer and pass it the back/forward jacobian\n",
    "        return self.previous_layer.back(back_jacobian)\n",
    "    \n",
    "    #Apply changes and flush the stores\n",
    "    def flush_updates(self, multiplier=1):\n",
    "        #Apply weight changes and clear weight_jacobian_store\n",
    "        self.weights -= multiplier * self.weight_jacobian_store\n",
    "        self.weight_jacobian_store *= 0\n",
    "        #Apply bias changes and clear bias_jacobian_store\n",
    "        self.biases -= multiplier * self.bias_jacobian_store\n",
    "        self.bias_jacobian_store *= 0\n",
    "        #Flush previous layer\n",
    "        return self.previous_layer.flush_updates(multiplier)\n",
    "        \n",
    "    #Weighted sum with previous activations, weights, and biases\n",
    "    def weighted_sum(self, previous_layer_activations):\n",
    "        return np.dot(self.weights, previous_layer_activations) + self.biases\n",
    "    \n",
    "    #Apply activation function\n",
    "    def activations(self, weighted_sum_result):\n",
    "        return self.activation_function(weighted_sum_result)\n",
    "        \n",
    "    #Calculate reusable intermediary jacobian from the forward/backward jacobian\n",
    "    def calculate_reusable_jacobian(self, forward_jacobian):\n",
    "        new_part = self.activation_function(self.weighted_sum_store, derivative=True)\n",
    "        #CHANGE to remove this unnecessary step\n",
    "        #new_part = np.einsum('i, ij ->ij', new_part, np.eye(np.array(new_part).shape[0]))\n",
    "        reusable_jacobian = np.einsum('a, ab', forward_jacobian, new_part)\n",
    "        return reusable_jacobian\n",
    "        \n",
    "    #Calculate weight jacobian using reusable jacobian\n",
    "    def calculate_weight_jacobian(self, reusable_jacobian):\n",
    "        new_part = np.einsum('ik, jl -> ijkl', np.eye(self.weights.shape[0]), np.eye(self.weights.shape[1]))\n",
    "        new_part = np.einsum('ijkl, j -> ikl', new_part, self.previous_activation_store)\n",
    "        weight_jacobian = np.einsum('a, abc -> bc', reusable_jacobian, new_part)\n",
    "        return weight_jacobian\n",
    "    \n",
    "    #Calculate bias jacobian using reusable jacobian\n",
    "    def calculate_bias_jacobian(self, reusable_jacobian):\n",
    "        bias_jacobian = reusable_jacobian\n",
    "        return bias_jacobian\n",
    "    \n",
    "    #Calculate back/forward jacobian using reusable jacobian\n",
    "    def calculate_back_jacobian(self, reusable_jacobian):\n",
    "        new_part = self.weights\n",
    "        back_jacobian = np.einsum('a, ab', reusable_jacobian, new_part)\n",
    "        return back_jacobian\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnInputLayer(nnLayer):\n",
    "    \n",
    "    #Input Layers do not have previous layers\n",
    "    def __init__(self, input_dim, output_dim, activation_function):\n",
    "        super(nnInputLayer, self).__init__(input_dim, output_dim, activation_function, None)\n",
    "        \n",
    "    #previous activations for input layers are just the model inputs\n",
    "    def forward(self, X):\n",
    "        self.previous_activation_store = X\n",
    "        self.weighted_sum_store = self.weighted_sum(self.previous_activation_store)\n",
    "        #print(\"weighted_sum: \" + str(self.weighted_sum_store))\n",
    "        #print(\"activations: \" + str(self.activations(self.weighted_sum_store)))\n",
    "        return self.activations(self.weighted_sum_store)\n",
    "    \n",
    "    #Input layers are the base case\n",
    "    def back(self, forward_jacobian):\n",
    "        #Calculate reusable jacobian\n",
    "        reusable = self.calculate_reusable_jacobian(forward_jacobian)\n",
    "        #Calculate weight jacobian and update\n",
    "        weight_jacobian = self.calculate_weight_jacobian(reusable)\n",
    "        #print(\"learning rate: \" + str(self.learning_rate))\n",
    "        #print(\"weight_jacobian: \" + str(weight_jacobian))\n",
    "        #print(\"scaled_weight_jacobian: \" + str(self.learning_rate * weight_jacobian))\n",
    "        #print(\"new_weights: \" + str(self.weights - self.learning_rate * weight_jacobian))\n",
    "        self.weight_jacobian_store += weight_jacobian\n",
    "        #Calculate bias jacobian and update\n",
    "        bias_jacobian = self.calculate_bias_jacobian(reusable)\n",
    "        #print(\"bias_jacobian: \" + str(bias_jacobian))\n",
    "        #print(\"scaled_bias_jacobian: \" + str(self.learning_rate * bias_jacobian))\n",
    "        #print(\"new_biases: \" + str(self.biases - self.learning_rate * bias_jacobian))\n",
    "        self.bias_jacobian_store += bias_jacobian\n",
    "        #Calculate back/forward jacobian\n",
    "        back_jacobian = self.calculate_back_jacobian(reusable)\n",
    "        #No plan for a return value as of yet\n",
    "        return True\n",
    "    \n",
    "    #Input layers are the base case\n",
    "    def flush_updates(self, multiplier=1):\n",
    "        #Apply weight changes and clear weight_jacobian_store\n",
    "        self.weights -= multiplier * self.weight_jacobian_store\n",
    "        self.weight_jacobian_store *= 0\n",
    "        #Apply bias changes and clear bias_jacobian_store\n",
    "        self.biases -= multiplier * self.bias_jacobian_store\n",
    "        self.bias_jacobian_store *= 0\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnOutputLayer(nnLayer):\n",
    "    \n",
    "    #Output Layers work directly with the cost gradient and not forward jacobians\n",
    "    def back(self, Y):\n",
    "        #Calculate error vector\n",
    "        error_vector = self.activations(self.weighted_sum_store) - Y\n",
    "        #Calculate cost gradient\n",
    "        cost_gradient = error_vector\n",
    "        #Calculate reusable using cost gradient\n",
    "        reusable = self.calculate_reusable_jacobian(cost_gradient)\n",
    "        #Calculate weight jacobian and update\n",
    "        weight_jacobian = self.calculate_weight_jacobian(reusable)\n",
    "        self.weight_jacobian_store += weight_jacobian\n",
    "        #Calculate bias jacobian and update\n",
    "        bias_jacobian = self.calculate_bias_jacobian(reusable)\n",
    "        self.bias_jacobian_store += bias_jacobian\n",
    "        #Calculate back/forward jacobian\n",
    "        back_jacobian = self.calculate_back_jacobian(reusable)\n",
    "        #Call back() on previous layer and pass it theback/forward jacobian\n",
    "        self.previous_layer.back(back_jacobian)\n",
    "        #No plan for a return value as of yet\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(V, derivative=False):\n",
    "    if derivative:\n",
    "        out = [1 if i >= 0 else 0 for i in V]\n",
    "        #CHANGE to return diag of this output\n",
    "        return np.einsum('i, ij -> ij', out, np.eye(np.array(out).shape[0]))\n",
    "    else:\n",
    "        return V * (V > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyRelu(V, derivative=False):\n",
    "    if derivative:\n",
    "        out = [1 if i >= 0 else 0.01 for i in V]\n",
    "        return np.einsum('i, ij -> ij', out, np.eye(np.array(out).shape[0]))\n",
    "    else:\n",
    "        return V * (V > 0) + 0.01 * V * (V <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(V, derivative=False):\n",
    "    if derivative:\n",
    "        #CHANGE to return diag of this answer\n",
    "        out = sigmoid(V) * (1 - sigmoid(V))\n",
    "        return np.einsum('i, ij -> ij', out, np.eye(np.array(out).shape[0]))\n",
    "    else:\n",
    "        return 1/(1 + np.exp(-V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(V, derivative=False):\n",
    "    if derivative:\n",
    "        S = softmax(V)\n",
    "        out = np.einsum('i, ik -> ik', S, np.eye(S.shape[0])) - np.einsum('i,k->ik', S, S)\n",
    "        return out\n",
    "    else:\n",
    "        E = np.exp(V)\n",
    "        return E / np.sum(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "number_of_samples = len(digits.images)\n",
    "data = digits.images.reshape((number_of_samples, -1))\n",
    "targets = digits.target\n",
    "#print(targets)\n",
    "onehot = np.zeros((len(targets), 10))\n",
    "onehot[np.arange(len(targets)), targets] = 1\n",
    "\n",
    "#data is an array of arrays\n",
    "#targets is an array of onehot arrays\n",
    "\n",
    "X_Array = data / 16.0\n",
    "Y_Array = onehot\n",
    "\n",
    "image_size = len(X_Array[0])\n",
    "print(\"Image size: \" + str(image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(number_of_samples * .8)\n",
    "\n",
    "X_Array_Train = X_Array[:split_point]\n",
    "Y_Array_Train = Y_Array[:split_point]\n",
    "targets_train = targets[:split_point]\n",
    "num_training_samples = len(X_Array_Train)\n",
    "\n",
    "X_Array_Test = X_Array[split_point:]\n",
    "targets_test = targets[split_point:]\n",
    "num_testing_samples = len(X_Array_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuralNetwork().create_homogenous_model([64, 16, 10], leakyRelu, learning_rate=0.01)\n",
    "#model.last_layer.activation_function = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.117606123869\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(0, num_training_samples):\n",
    "    if np.argmax(model.forward(X_Array_Train[i])) == targets_train[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Training Set Accuracy: \" + str(float(correct) / num_training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy: 0.119444444444\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(0, num_testing_samples):\n",
    "    if np.argmax(model.forward(X_Array_Test[i])) == targets_test[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Testing Set Accuracy: \" + str(float(correct) / num_testing_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(X_Array_Train, Y_Array_Train, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.976339596381\n",
      "[1. 4. 0. 9. 1. 6. 3. 0. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong_counter = np.zeros(10)\n",
    "for i in range(0, num_training_samples):\n",
    "    if np.argmax(model.forward(X_Array_Train[i])) == targets_train[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong_counter[targets_train[i]] += 1\n",
    "        \n",
    "print(\"Training Set Accuracy: \" + str(float(correct) / num_training_samples))\n",
    "print(wrong_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Accuracy: 0.891666666667\n",
      "[ 2.  7.  0. 13.  3.  0.  2.  2.  3.  7.]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong_counter = np.zeros(10)\n",
    "for i in range(0, num_testing_samples):\n",
    "    if np.argmax(model.forward(X_Array_Test[i])) == targets_test[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong_counter[targets_test[i]] += 1\n",
    "        \n",
    "print(\"Testing Set Accuracy: \" + str(float(correct) / num_testing_samples))\n",
    "print(wrong_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 0\n",
    "print(targets[case])\n",
    "print(np.argmax(model.forward(X_Array[case])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
